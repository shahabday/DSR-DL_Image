{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps:\n",
    "- Understand the Momentum hyperparameter\n",
    "- Search the space of hyperparameters\n",
    "- Saving, checkpointing and loading your models\n",
    "- Apply learning rate schedulers to our existing networks\n",
    "- Debugging deep learning models\n",
    "\n",
    "### Covered topics and learning objectives\n",
    "- Playing with parameters. Momentum\n",
    "- Saving, checkpointing and loading your models\n",
    "- Learning Rate schedulers\n",
    "- Debugging deep learning models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvNets Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing with parameters. Momentum\n",
    "\n",
    "**Momentum:** You already learned about backpropagation and gradient decent, so we will not cover this here. \n",
    "Remember that some optimizers use momentum because it is a very effective optimization approach: Instead of using only the gradient of the current step to guide the search, momentum also accumulates the gradient of the past steps to determine the direction to go.\n",
    "\n",
    "Additional Resources (optional reads):\n",
    "- [Parameters vs. hyperparameters](https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/)\n",
    "- [Momentum d2l](https://d2l.ai/chapter_optimization/momentum.html)\n",
    "- [Momentum blog paperspace](https://blog.paperspace.com/intro-to-optimization-momentum-rmsprop-adam/)\n",
    "\n",
    "Momentum is just one example of a hyperparameter that we can choose. \n",
    "Let's learn about how to optimize hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Keras tuner for hyperparameter optimization \n",
    "\n",
    "Blindly trying out different architecture configurations works well enough if you just need something that works okay. In this section, we’ll go beyond \"works okay\", to \"works great and wins machine-learning competitions\", via a quick guide to a set of must-know techniques for building state-of-the-art deep-learning models.\n",
    "\n",
    "#### Hyperparameter optimization\n",
    "\n",
    "When building a deep-learning model, you have to make many seemingly arbitrary decisions: How many layers should you stack? How many units or filters should go in each layer? Should you use relu as activation, or a different function? Should you use BatchNormalization after a given layer? How much dropout should you use? And so on. These architecture-level parameters are called hyperparameters to distinguish them from the parameters of a model, which are trained via backpropagation.\n",
    "\n",
    "In practice, experienced machine-learning engineers and researchers build intuition over time as to what works and what doesn’t when it comes to these choices—they develop hyperparameter-tuning skills. But there are no formal rules. If you want to get to the very limit of what can be achieved on a given task, you can’t be content with such arbitrary choices. Your initial decisions are almost always suboptimal, even if you have very good intuition. You can refine your choices by tweaking them by hand and retraining the model repeatedly—that’s what machine-learning engineers and researchers spend most of their time doing. But it shouldn’t be your job as a human to fiddle with hyperparameters all day—that is better left to a machine.\n",
    "\n",
    "Thus you need to explore the space of possible decisions automatically, systematically, in a principled way. You need to search the architecture space and find the best-performing ones empirically. That’s what the field of automatic hyperparameter optimization is about: it’s an entire field of research, and an important one.\n",
    "\n",
    "The process of optimizing hyperparameters typically looks like this:\n",
    "\n",
    "1. Choose a set of hyperparameters (automatically).\n",
    "1. Build the corresponding model.\n",
    "1. Fit it to your training data, and measure performance on the validation data.\n",
    "1. Choose the next set of hyperparameters to try (automatically).\n",
    "1. Repeat.\n",
    "1.Eventually, measure performance on your test data.\n",
    "\n",
    "The key to this process is the algorithm that analyzes the relationship between validation performance and various hyperparameter values to choose the next set of hyperparameters to evaluate. Many different techniques are possible: Bayesian optimization, genetic algorithms, simple random search, and so on.\n",
    "\n",
    "Training the weights of a model is relatively easy: you compute a loss function on a mini-batch of data and then use backpropagation to move the weights in the right direction. Updating hyperparameters, on the other hand, presents unique challenges. Consider the following:\n",
    "\n",
    "* The hyperparameter space is typically made of discrete decisions and thus isn’t continuous or differentiable. Hence, you typically can’t do gradient descent in hyperparameter space. Instead, you must rely on gradient-free optimization techniques, which naturally are far less efficient than gradient descent.\n",
    "* Computing the feedback signal of this optimization process (does this set of hyperparameters lead to a high-performing model on this task?) can be extremely expensive: it requires creating and training a new model from scratch on your dataset.\n",
    "* The feedback signal may be noisy: if a training run performs 0.2% better, is that because of a better model configuration, or because you got lucky with the initial weight values?\n",
    "\n",
    "Thankfully, there’s a tool that makes hyperparameter tuning simpler: KerasTuner. Let’s check it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key idea that KerasTuner is built upon is to let you replace hard-coded hyperparameter values, such as units=32, with a range of possible choices, such as Int(name=\"units\", min_value=16, max_value=64, step=16). The set of such choices in a given model is called the search space of the hyperparameter tuning process.\n",
    "\n",
    "To specify a search space, define a model-building function. It takes a hp argument, from which you can sample hyperparameter ranges, and it returns a compiled Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kerastuner as kt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def build_model(hp):\n",
    "    units = hp.Int(name=\"units\", min_value=16, max_value=64, step=16)\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(units, activation=\"relu\"),\n",
    "        layers.Dense(10, activation=\"softmax\")\n",
    "    ])\n",
    "    optimizer = hp.Choice(name=\"optimizer\", values=[\"rmsprop\", \"adam\"])\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to define a \"tuner\". Schematically, you can think of a tuner as for loop, which will repeatedly:\n",
    "\n",
    "* Pick a set of hyperparameter values.\n",
    "* Call the model-building function with these values to create a model.\n",
    "* Train the model and record its metrics.\n",
    "\n",
    "KerasTuner has several built-in tuners available—RandomSearch, BayesianOptimization, Hyperband. Let’s try BayesianOptimization, a tuner that attempts to make smart predictions for which new hyperparameter values are likely to perform best given the outcome of previous choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.BayesianOptimization(\n",
    "    build_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=4,\n",
    "    executions_per_trial=2,\n",
    "    directory=\"mnist_kt_test\",\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "# You can display an overview of the search space via the following\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape((-1, 28 * 28)).astype(\"float32\") / 255\n",
    "x_test = x_test.reshape((-1, 28 * 28)).astype(\"float32\") / 255\n",
    "x_train_full = x_train[:]\n",
    "y_train_full = y_train[:]\n",
    "num_val_samples = 10000\n",
    "x_train, x_val = x_train[:-num_val_samples], x_train[-num_val_samples:]\n",
    "y_train, y_val = y_train[:-num_val_samples], y_train[-num_val_samples:]\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5),\n",
    "]\n",
    "tuner.search(\n",
    "    x_train, y_train,\n",
    "    batch_size=128,\n",
    "    epochs=3,\n",
    "    validation_data=(x_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Print out the best hyperparameters!\n",
    "\n",
    "Hints: To do so, you can use the `tuner` object's tuner `get_best_hyperparameters` method. \n",
    "You will get a list of hyperparameters, each of which has a `values` instance variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "top_n = 4\n",
    "best_hps = tuner.get_best_hyperparameters(top_n)\n",
    "for my_hp in best_hps:\n",
    "    print(my_hp.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "How to set the following variables in a good way?\n",
    "\n",
    "- `epochs`\n",
    "- `patience`\n",
    "- `max_trials`\n",
    "- `executions_per_trial`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTION**\n",
    "\n",
    "Note: these might not be perfect solutions, but they can help to build a basic intuition/explanation.\n",
    "\n",
    "A neural net trained from scratch probably needs a few epochs to get trained successfully. `epochs >= 15`\n",
    "\n",
    "With 15 epochs in total, it is probably fine to set `patience = 5`\n",
    "\n",
    "There are 8 (2 optimizers times 4 options for units) different trials possible, running 8 trials is still affordable, therefore `max_trials >= 8`.\n",
    "\n",
    "Training on MNIST is pretty stable/robust, therefore `executions_per_trial = 1` would probably be enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "In Module 1.1, we defined a network using the RMSprop optimizer. \n",
    "\n",
    "Go back to Module 1.1 and try to find hyperparameters (learning rate and momentum) which yield a better model.\n",
    "\n",
    "How could we be sure that we found the best hyperparameters?"
   ]
  },
  {
   "source": [
    "**SOLUTION**\n",
    "\n",
    "Here is the code for the optimizer:\n",
    "\n",
    "```python\n",
    "opt = keras.optimizers.RMSprop(\n",
    "    learning_rate=0.001,\n",
    "    momentum=0.0,\n",
    ")\n",
    "model.compile(optimizer=opt,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "In module 1.1.:\n",
    "- accuracy with defaults(`learning_rate=0.001`, `momentum=0.0`): `0.992%`, `0.991%`\n",
    "- momentum 0.3: `0.9924`\n",
    "\n",
    "Running kerastuner:\n",
    "\n",
    "```\n",
    "import kerastuner as kt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def build_model(hp):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    opt = keras.optimizers.RMSprop(\n",
    "        learning_rate=hp.Choice(name=\"learning_rate\", values=[0.0001, 0.001, 0.01]),  # 0.001,\n",
    "        momentum=hp.Choice(name=\"momentum\", values=[0.0, 0.3, 0.6, 0.9]),  # 0.0 # 0.3,\n",
    "    )\n",
    "    model.compile(optimizer=opt,\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    build_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=12,\n",
    "    executions_per_trial=1,\n",
    "    directory=\"../data/mnist_kt_test\",\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "tuner.search(\n",
    "    train_images, train_labels,\n",
    "    batch_size=64,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(test_images, test_labels),\n",
    "    verbose=2,\n",
    ")\n",
    "```\n",
    "\n",
    "After running kerastuner you get an output like this: \n",
    "\n",
    "```\n",
    "val_accuracy: 0.9918000102043152\n",
    "Best val_accuracy So Far: 0.9922999739646912\n",
    "```\n",
    "\n",
    "After running:\n",
    "\n",
    "```\n",
    "top_n = 1\n",
    "best_hps = tuner.get_best_hyperparameters(top_n)\n",
    "for my_hp in best_hps:\n",
    "    print(my_hp.values)\n",
    "```\n",
    "\n",
    "you get a parameter combination like this:\n",
    "\n",
    "This means that the `{'learning_rate': 0.001, 'momentum': 0.3}` are a good hyperparameter choice. \n",
    "However the result is not much different from what we got in Module1.1.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** \n",
    "\n",
    "Which hyperparameters can we tune in the ConvNets we have used so far in `Module 1`, `Module 2`, and `Module 3`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOLUTION:**\n",
    "\n",
    "- learning rate\n",
    "- momentum\n",
    "- Neural architecture\n",
    "    - number of layers\n",
    "    - number of filter maps\n",
    "    - number of units per layer\n",
    "- loss function\n",
    "- optimizer\n",
    "- expected input image size\n",
    "- how strongly do we apply data augmentations\n",
    "- when transfer-learning and/or fine-tuning: how many layers to freeze/fine-tune\n",
    "- which learning rate scheduler to use\n",
    "- strength (a.k.a. probability) of dropout\n",
    "\n",
    "We generally could tune the following but the research already shows clear rules how to pick the following (so hyperparameter search is often unnecessary)\n",
    "- kernel size\n",
    "- batch size\n",
    "- which data augmentations to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving, checkpointing and loading your models\n",
    "\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "Do the tutorial: https://www.tensorflow.org/tutorials/keras/save_and_load. \n",
    "\n",
    "Then apply the learnings to your own model from Module 1:\n",
    "- Save checkpoints during training\n",
    "- Load the best-performing checkpoint from disk. \n",
    "- Evaluate your loaded model and confirm that the performance is the same as during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdQnlA23RWaH"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWE-y16aRWaJ"
   },
   "source": [
    "# Learning Rate Schedulers\n",
    "\n",
    "Go to https://d2l.ai/chapter_optimization/lr-scheduler.html\n",
    "and read this chapter on learning rate schedulers. \n",
    "\n",
    "**Exercise**: Change the cats-vs-dogs code such that it uses the learning rate schedulers. \n",
    "Which scheduler works best?\n",
    "* Let's use the model in the end of 1.2., a convnet with data augmentation. \n",
    "* Let's say you have a limited resource of 20 epochs per try\n",
    "\n",
    "Additional Resources (reading is optional):\n",
    "- https://keras.io/api/callbacks/learning_rate_scheduler/\n",
    "- https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION LR scheduler\n",
    "import math\n",
    "\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "EPOCHS = 20\n",
    "LR = 0.001\n",
    "\n",
    "def get_checkpoint_callback():\n",
    "    return keras.callbacks.ModelCheckpoint(\n",
    "      filepath=\"convnet_from_scratch_with_augmentation_lr_scheduler.keras\",\n",
    "      save_best_only=True,\n",
    "      monitor=\"val_loss\")\n",
    "\n",
    "def create_model():\n",
    "    inputs = keras.Input(shape=(180, 180, 3))\n",
    "    x = data_augmentation(inputs)\n",
    "    x = layers.experimental.preprocessing.Rescaling(1./255)(x)\n",
    "    x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
    "    x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "    x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "    x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "    x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "    x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "    x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "    x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "    x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquareRootScheduler:\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def __call__(self, num_update):\n",
    "        return self.lr * pow(num_update + 1.0, -0.5)\n",
    "scheduler = SquareRootScheduler(lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.plot(tf.range(EPOCHS), [scheduler(t) for t in range(EPOCHS)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_scheduler(scheduler):\n",
    "\n",
    "    checkpoint_callback = get_checkpoint_callback()\n",
    "    callbacks = [checkpoint_callback, LearningRateScheduler(scheduler),]\n",
    "    opt = keras.optimizers.RMSprop(learning_rate=lr)\n",
    "    model = create_model()\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(train_dataset, epochs=EPOCHS,\n",
    "        validation_data=validation_dataset, callbacks=callbacks)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = train_with_scheduler(scheduler)"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "By running with different schedulers, we achieve the following training behavior:\n",
    "\n",
    "\n",
    "SquareRoot scheduler:\n",
    "<img src=\"./solution_assets/square_root.png\" alt=\"square_root\" style=\"width: 200px;\"/>\n",
    "\n",
    "Cosine scheduler:\n",
    "<img src=\"./solution_assets/cosine.jpg\" alt=\"cosine\" style=\"width: 200px;\"/>\n",
    "\n",
    "Cosine scheduler with warmup:\n",
    "<img src=\"./solution_assets/cosine_with_warmup.jpg\" alt=\"cosine_with_warmup\" style=\"width: 200px;\"/>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging deep learning models\n",
    "\n",
    "We plan that this section will take approx. 2 hours for studying materals\n",
    "\n",
    "https://github.com/stanislav-chekmenev/debugging-dl-models\n",
    "\n",
    "* `notebooks/basics/0_intro.ipynb`: Read/Skim\n",
    "* `notebooks/basics/1_implement_a_bug_free_model.ipynb`: Read\n",
    "* `notebooks/basics/2_most_common_bugs_I.ipynb`: Read and do exercise in the bottom\n",
    "* `notebooks/basics/3_most_common_bugs_II.ipynb`: Read and do exercise in the bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e88cacac4a4e81780274e5b67662f71286bfdfe71b49b67699dc84b91a2b06f4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}